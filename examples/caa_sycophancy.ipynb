{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Activation Addition\n",
    "\n",
    "This notebook aims to reproduce the workflow defined in [Contrastive Activation Addition](https://arxiv.org/abs/2312.06681) for extracting steering vectors from input. The official codebase can be found [here](https://github.com/nrimsky/CAA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install steering-vectors\n",
    "# For loading in 8-bit precision\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be consistent with CAA, we run on Llama-2 chat models of sizes 7b and 13b. These can be downloaded through Huggingface Transformers but require you to have first applied for access [here](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def get_model_and_tokenizer(model_name: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Note: you must have installed 'accelerate', 'bitsandbytes' to load in 8bit\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, load_in_8bit=True\n",
    "    )\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"13b\" # or \"7b\"\n",
    "model_name = f\"meta-llama/Llama-2-{model_size}-chat-hf\"\n",
    "model, tokenizer = get_model_and_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demonstration, we'll download the sycophancy train and test split used in the CAA paper. CAA uses data formatted in the style of Anthropic's Model-Written Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the CAA dataset\n",
    "!wget https://raw.githubusercontent.com/nrimsky/CAA/main/preprocessed_data/generate_dataset.json\n",
    "!wget https://raw.githubusercontent.com/nrimsky/CAA/main/preprocessed_data/test_dataset.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Define a shorthand type for model-written eval datum\n",
    "MWEData = list[dict[str, str]]\n",
    "\n",
    "train_data: list[MWEData] = json.load(open(\"generate_dataset.json\", 'r'))\n",
    "test_data: list[MWEData] = json.load(open(\"test_dataset.json\", 'r'))\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)\n",
    "train_data = train_data\n",
    "test_data = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST = \"[INST]\"\n",
    "E_INST = \"[/INST]\"\n",
    "SYS_PROMPT = \"You are a helpful, honest assistant.\"\n",
    "B_SYS = \"<<SYS>>\"\n",
    "E_SYS = \"<</SYS>>\"\n",
    "BOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "# Note: No EOS token is used in original codebase\n",
    "prompt_template = f\"{BOS_TOKEN} {B_INST} \\n{B_SYS}{SYS_PROMPT}{E_SYS}\\n\\n {{question}} {E_INST} {{answer}}\"\n",
    "\n",
    "def make_pos_neg_pair(mwe_data: MWEData) -> tuple[str, str]:\n",
    "    \"\"\" Creates a (positive, negative) pair for getting contrastive activations \"\"\"\n",
    "    pos = prompt_template.format(\n",
    "        question = mwe_data['question'],\n",
    "        answer = mwe_data['answer_matching_behavior']   \n",
    "    )\n",
    "    neg = prompt_template.format(\n",
    "        question = mwe_data['question'],\n",
    "        answer = mwe_data['answer_not_matching_behavior']\n",
    "    )\n",
    "    return pos, neg\n",
    "\n",
    "def make_dataset(list_mwe_data: list[MWEData]) -> list[tuple[str, str]]:\n",
    "    \"\"\" Creates a list of (positive, negative) pairs for getting contrastive activations \"\"\"\n",
    "    return [make_pos_neg_pair(mwe_data) for mwe_data in list_mwe_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_data)\n",
    "test_dataset = make_dataset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one example from the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, neg = train_dataset[0]\n",
    "print(\"#### Positive Prompt ####\")\n",
    "print(pos)\n",
    "print()\n",
    "print(\"#### Negative Prompt ####\")\n",
    "print(neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Without Steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll define some utility code to: \n",
    "1. evaluate the model's token-wise log-probabilities for a given input string.\n",
    "2. convert the unnormalized probabilities for each MCQ answer to a normalized probability distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import PreTrainedTokenizerBase as Tokenizer\n",
    "from transformers import PreTrainedModel as Model\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable\n",
    "\n",
    "def get_probabilities(logprobs: list[float]) -> list[float]:\n",
    "    \"\"\" Converts log-probabilities to a normalized probability distribution \"\"\"\n",
    "    min_logprob = min(logprobs)\n",
    "    # Shift the range to avoid underflow when exponentiating\n",
    "    logprobs = [logprob - min_logprob for logprob in logprobs]\n",
    "    # Exponentiate and normalize\n",
    "    probs = [math.exp(logprob) for logprob in logprobs]\n",
    "    total = sum(probs)\n",
    "    probs = [prob / total for prob in probs]\n",
    "    return probs\n",
    "\n",
    "@dataclass\n",
    "class TokenProb:\n",
    "    token_id: int\n",
    "    logprob: float\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class TextProbs:\n",
    "    text: str\n",
    "    token_probs: list[TokenProb]\n",
    "\n",
    "    @property\n",
    "    def sum_logprobs(self) -> float:\n",
    "        return sum([tp.logprob for tp in self.token_probs])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"TextProbs({self.text}:{self.sum_logprobs:.2f})\"\n",
    "\n",
    "def get_text_probs(input: str, model: Model, tokenizer: Tokenizer, ) -> TextProbs:\n",
    "    \"\"\" Get the token-wise probabilities of a given input \"\"\"\n",
    "    inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, output_hidden_states=False, return_dict=True)\n",
    "    logprobs = torch.log_softmax(outputs.logits, dim=-1).detach().cpu()\n",
    "    # collect the probability of the generated token -- probability at index 0 corresponds to the token at index 1\n",
    "    logprobs = logprobs[:, :-1, :]\n",
    "    target_ids = inputs.input_ids[:, 1:]\n",
    "    # Get the probability of the subsequent token\n",
    "    gen_logprobs = torch.gather(logprobs, 2, target_ids[:, :, None]).squeeze(-1)[0]\n",
    "\n",
    "    text_logprobs: list[TokenProb] = []\n",
    "    for token, p in zip(target_ids[0], gen_logprobs):\n",
    "        if token not in tokenizer.all_special_ids:\n",
    "            text_logprobs.append(\n",
    "                TokenProb(\n",
    "                    token_id=token.item(),\n",
    "                    text=tokenizer.decode(token),\n",
    "                    logprob=p.item(),\n",
    "                )\n",
    "            )\n",
    "    return TextProbs(text=input, token_probs=text_logprobs)\n",
    "    \n",
    "\n",
    "def evaluate_model(\n",
    "    model: Model, \n",
    "    tokenizer: Tokenizer, \n",
    "    dataset: Iterable[tuple[str, str]],\n",
    "    show_progress: bool = False\n",
    "):\n",
    "    \"\"\" Evaluate model on dataset and return normalized probability of correct answer \"\"\"\n",
    "    total_pos_prob = 0.0\n",
    "    for pos_prompt, neg_prompt in tqdm(dataset, disable=not show_progress, desc=\"Evaluating\"):\n",
    "        pos: TextProbs = get_text_probs(pos_prompt, model, tokenizer)\n",
    "        neg: TextProbs = get_text_probs(neg_prompt, model, tokenizer)\n",
    "        # NOTE: We compare logprobs of the full (prompt + response).  \n",
    "        # This is equivalent to comparing response log-probs only.  \n",
    "        # Because the prompts are the same for both positive and negative, \n",
    "        # the prompt log-probs factor out as an additive constant in the total log-probs.\n",
    "        # and so the relative difference in log-probs is unchanged.\n",
    "        pos_prob, _ = get_probabilities([pos.sum_logprobs, neg.sum_logprobs])\n",
    "        total_pos_prob += pos_prob\n",
    "    return total_pos_prob / len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of `evaluate_model` is the average probability of picking the sycophantic answer over the non-sycophantic answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(dtch1996): current implementation is slow...\n",
    "result = evaluate_model(model, tokenizer, test_dataset, show_progress=True)\n",
    "print(f\"Unsteered model: {result:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Steering Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from steering_vectors import train_steering_vector, SteeringVector\n",
    "\n",
    "steering_vector: SteeringVector = train_steering_vector(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    train_dataset,\n",
    "    move_to_cpu=True,\n",
    "    # NOTE: You can specify a list[int] of desired layer indices\n",
    "    # If layers is None, then all layers are used\n",
    "    # Here, layer 15 is the layer where sycophancy steering worked best in the CAA paper\n",
    "    # for both Llama-2-7b-chat and Llama-2-13b-chat. \n",
    "    layers = [15], \n",
    "    # NOTE: The second last token corresponds to the A/B position\n",
    "    # which is where we believe the model makes its decision \n",
    "    read_token_index=-2,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(steering_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sanity check our vector by evaluating the cosine similarity with the ground truth sycophancy vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the CAA sycophancy vectors for layer 15\n",
    "!wget https://raw.githubusercontent.com/nrimsky/CAA/main/vectors/vec_layer_15_Llama-2-7b-chat-hf.pt\n",
    "!wget https://raw.githubusercontent.com/nrimsky/CAA/main/vectors/vec_layer_15_Llama-2-13b-chat-hf.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "original_steering_vector = torch.load(f'vec_layer_15_Llama-2-{model_size}-chat-hf.pt')\n",
    "our_steering_vector = steering_vector.layer_activations[15]\n",
    "print(f\"Cosine similarity: {cosine_similarity(original_steering_vector, our_steering_vector, dim=0):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steer with Steering Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply steering vectors with `SteeringVector.apply` as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for multiplier in (-1, 0, 1):\n",
    "    with steering_vector.apply(model, multiplier=multiplier, min_token_index=0):\n",
    "        # Within the scope, model activations are modified\n",
    "        result = evaluate_model(model, tokenizer, test_dataset)\n",
    "        print(f\"{multiplier} steered model: {result:.3f}\")\n",
    "        # Upon leaving the scope, original model activations are restored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: PyTorch Hooks\n",
    "To apply steering vectors to the model, we must have some way of modifying the model's forward pass. The approach taken in the official codebases for Representation Engineering and Contrastive Activation Addition both do this by wrapping the model's blocks. This approach is conceptually simpler but has notable limitations, e.g. being unable to work for other models\n",
    " \n",
    "The `steering_vectors` library uses Pytorch hooks instead of model wrappers to modify the underlying model's forward pass. You can read more about hooks at the [official PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
